{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rzyqc7Y58vKU",
    "outputId": "7f0b739f-be80-4cb4-dcda-6e985d4434be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/bittu/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/bittu/.local/lib/python3.10/site-packages (2.2.5)\n",
      "Requirement already satisfied: matplotlib in /home/bittu/.local/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in /home/bittu/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: wordcloud in /home/bittu/.local/lib/python3.10/site-packages (1.9.4)\n",
      "Requirement already satisfied: scikit-learn in /home/bittu/.local/lib/python3.10/site-packages (1.6.1)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/bittu/.local/lib/python3.10/site-packages (1.15.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/bittu/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/bittu/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/lib/python3/dist-packages (from matplotlib) (4.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bittu/.local/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/bittu/.local/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/bittu/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/bittu/.local/lib/python3.10/site-packages (from scikit-learn) (1.5.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorflow) (2.25.1)\n",
      "Collecting h5py>=3.11.0\n",
      "  Downloading h5py-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard~=2.19.0\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras>=3.5.0\n",
      "  Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes<1.0.0,>=0.5.1\n",
      "  Downloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/bittu/.local/lib/python3.10/site-packages (from tensorflow) (5.29.4)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/bittu/.local/lib/python3.10/site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/bittu/.local/lib/python3.10/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting namex\n",
      "  Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting optree\n",
      "  Downloading optree-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (405 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.8/405.8 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rich\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, pygments, optree, opt-einsum, numpy, mdurl, MarkupSafe, markdown, google-pasta, gast, astunparse, absl-py, werkzeug, ml-dtypes, markdown-it-py, h5py, tensorboard, rich, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.5\n",
      "    Uninstalling numpy-2.2.5:\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.3.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 h5py-3.13.0 keras-3.10.0 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 numpy-2.1.3 opt-einsum-3.4.0 optree-0.16.0 pygments-2.19.1 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib seaborn wordcloud scikit-learn tensorflow scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "up6wsnwA9mYj"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# export TF_CPP_MIN_LOG_LEVEL=2\n",
    "# export TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mULWL6d9mat",
    "outputId": "15223942-6590-4370-fb7d-15cdd5f79d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "TPU initialization failed: COLAB_TPU_ADDR environment variable not set. Ensure TPU runtime is selected.\n"
     ]
    }
   ],
   "source": [
    "# Enable TPU with diagnostics\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "try:\n",
    "    tpu_address = os.environ.get('COLAB_TPU_ADDR')\n",
    "    if not tpu_address:\n",
    "        raise ValueError(\"COLAB_TPU_ADDR environment variable not set. Ensure TPU runtime is selected.\")\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + tpu_address)\n",
    "    print(f\"TPU endpoint: {tpu.cluster_spec().as_dict()}\")\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    print(\"TPU initialized successfully\")\n",
    "except ValueError as e:\n",
    "    print(f\"TPU initialization failed: {e}\")\n",
    "    strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLdtqIU79mdG"
   },
   "outputs": [],
   "source": [
    "# Load and preprocess data in chunks\n",
    "csv_file = '/TMDB_movie_dataset_v11.csv'  # Update with your local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vcKxiFHx_hdn"
   },
   "outputs": [],
   "source": [
    "chunk_size = 100000\n",
    "chunks = pd.read_csv(csv_file, chunksize=chunk_size, low_memory=False)\n",
    "df_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bQg63h1T9mkJ"
   },
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    chunk = chunk.fillna('')\n",
    "    chunk['text'] = chunk.astype(str).agg(' '.join, axis=1)\n",
    "    chunk['PrimaryGenre'] = chunk['genres'].str.split(',').str[0].str.strip()\n",
    "    chunk['PrimaryGenre'] = chunk['PrimaryGenre'].replace('', 'Unknown')\n",
    "    df_chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xIvMiYJk9mmt"
   },
   "outputs": [],
   "source": [
    "df = pd.concat(df_chunks, ignore_index=True)\n",
    "top_genres = df['PrimaryGenre'].value_counts().head(10).index\n",
    "df['PrimaryGenre'] = df['PrimaryGenre'].where(df['PrimaryGenre'].isin(top_genres), 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmzShpaT9mpG",
    "outputId": "1dae8965-d7f9-4f72-f182-9995af3abe2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached TF-IDF matrix: (1231678, 20000)\n"
     ]
    }
   ],
   "source": [
    "# --- Load or compute TF-IDF matrix ---\n",
    "tfidf_cache = 'tfidf_matrix.npz'\n",
    "if os.path.exists(tfidf_cache):\n",
    "    tfidf_matrix = sp.load_npz(tfidf_cache)\n",
    "    print(f\"Loaded cached TF-IDF matrix: {tfidf_matrix.shape}\")\n",
    "else:\n",
    "    tfidf = TfidfVectorizer(stop_words='english', max_features=20000, min_df=5)\n",
    "    tfidf_matrix = tfidf.fit_transform(df['text']).astype(np.float32)\n",
    "    sp.save_npz(tfidf_cache, tfidf_matrix)\n",
    "    print(f\"Computed and saved TF-IDF matrix: {tfidf_matrix.shape}\")\n",
    "feature_names = tfidf.get_feature_names_out() if 'tfidf' in locals() else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9O0tjvqg9mrp"
   },
   "outputs": [],
   "source": [
    "# --- Convert sparse matrix to tf.SparseTensor ---\n",
    "def sparse_to_tensor(sparse_matrix):\n",
    "    coo = sparse_matrix.tocoo()\n",
    "    indices = np.vstack((coo.row, coo.col)).transpose()\n",
    "    values = coo.data\n",
    "    shape = coo.shape\n",
    "    return tf.SparseTensor(indices=indices, values=values, dense_shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MpfzAT2h9muT"
   },
   "outputs": [],
   "source": [
    "# --- Compute cosine similarity in batches ---\n",
    "def compute_cosine_similarity_local(tfidf_matrix, batch_size=1024):\n",
    "    n_samples = tfidf_matrix.shape[0]\n",
    "    tfidf_tensor = sparse_to_tensor(tfidf_matrix)\n",
    "    tfidf_dense = tf.sparse.to_dense(tfidf_tensor)  # Convert to dense tensor\n",
    "\n",
    "    def batch_cosine_sim(batch, matrix):\n",
    "        norms_batch = tf.linalg.norm(batch, axis=1, keepdims=True)\n",
    "        norms_matrix = tf.linalg.norm(matrix, axis=1, keepdims=True)\n",
    "        normalized_batch = batch / (norms_batch + 1e-10)\n",
    "        normalized_matrix = matrix / (norms_matrix + 1e-10)\n",
    "        return tf.matmul(normalized_batch, normalized_matrix, transpose_b=True)\n",
    "\n",
    "    sim_file = 'cosine_sim_chunks'\n",
    "    os.makedirs(sim_file, exist_ok=True)\n",
    "    chunk_index = 0\n",
    "\n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch = tfidf_dense[start:end]\n",
    "        sim_batch = batch_cosine_sim(batch, tfidf_dense)\n",
    "        np.save(f'{sim_file}/sim_chunk_{chunk_index}.npy', sim_batch.numpy())\n",
    "        chunk_index += 1\n",
    "        print(f\"Saved similarity chunk {chunk_index} for rows {start} to {end}\")\n",
    "\n",
    "    cosine_sim = []\n",
    "    for i in range(chunk_index):\n",
    "        chunk = np.load(f'{sim_file}/sim_chunk_{i}.npy')\n",
    "        cosine_sim.append(chunk)\n",
    "    return np.concatenate(cosine_sim, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDtvOLwe9mwt",
    "outputId": "bac8e923-33d3-4703-9e23-ea4e52f2f9db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 17:15:28.952142: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (mklcpu) ran out of memory trying to allocate 91.77GiB (rounded to 98534240000)requested by op SparseToDense\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-05-29 17:15:28.952161: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1058] BFCAllocator dump for mklcpu\n",
      "2025-05-29 17:15:28.952164: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952165: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952167: I externa"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__SparseToDense_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[1231678,20000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:SparseToDense] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12453/111610610.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded cached cosine similarity matrix: {cosine_sim.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cosine_similarity_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_sim_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Computed and saved cosine similarity matrix: {cosine_sim.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12453/371123220.py\u001b[0m in \u001b[0;36mcompute_cosine_similarity_local\u001b[0;34m(tfidf_matrix, batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtfidf_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtfidf_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_tensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to dense tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_cosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/sparse_ops.py\u001b[0m in \u001b[0;36msparse_tensor_to_dense\u001b[0;34m(sp_input, default_value, validate_indices, name)\u001b[0m\n\u001b[1;32m   1726\u001b[0m     \u001b[0mdefault_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msp_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1728\u001b[0;31m   return gen_sparse_ops.sparse_to_dense(\n\u001b[0m\u001b[1;32m   1729\u001b[0m       \u001b[0msp_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m       \u001b[0msp_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_sparse_ops.py\u001b[0m in \u001b[0;36msparse_to_dense\u001b[0;34m(sparse_indices, output_shape, sparse_values, default_value, validate_indices, name)\u001b[0m\n\u001b[1;32m   3227\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3228\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3229\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3230\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6006\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__SparseToDense_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[1231678,20000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:SparseToDense] name: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952168: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952169: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952170: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952171: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952172: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952173: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952175: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952176: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952177: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952178: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952179: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952180: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952181: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952182: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952190: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952191: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952193: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (134217728): \tTotal Chunks: 3, Chunks in use: 2. 475.66MiB allocated for chunks. 274.17MiB in use in bin. 274.17MiB client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952195: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 2. 1.54GiB allocated for chunks. 1.07GiB in use in bin. 1.07GiB client-requested in use in bin.\n",
      "2025-05-29 17:15:28.952197: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1081] Bin for 91.77GiB was 256.00MiB, Chunk State: \n",
      "2025-05-29 17:15:28.952201: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1087]   Size: 475.66MiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 548.34MiB | Requested Size: 548.34MiB | in_use: 1 | bin_num: -1\n",
      "2025-05-29 17:15:28.952202: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1094] Next region of size 1073741824\n",
      "2025-05-29 17:15:28.952204: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 7bda51ba7040 of size 574972416 next 4\n",
      "2025-05-29 17:15:28.952205: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] Free  at 7bda73ffd240 of size 498769408 next 18446744073709551615\n",
      "2025-05-29 17:15:28.952206: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1094] Next region of size 1073741824\n",
      "2025-05-29 17:15:28.952207: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 7bdab3fff040 of size 574972416 next 1\n",
      "2025-05-29 17:15:28.952208: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 7bdad6455240 of size 143743232 next 2\n",
      "2025-05-29 17:15:28.952209: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 7bdaded6ab40 of size 143743232 next 5\n",
      "2025-05-29 17:15:28.952210: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] Free  at 7bdae7680440 of size 211282944 next 18446744073709551615\n",
      "2025-05-29 17:15:28.952210: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1119]      Summary of in-use Chunks by size: \n",
      "2025-05-29 17:15:28.952212: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 2 Chunks of size 143743232 totalling 274.17MiB\n",
      "2025-05-29 17:15:28.952213: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 2 Chunks of size 574972416 totalling 1.07GiB\n",
      "2025-05-29 17:15:28.952214: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1126] Sum Total of in-use chunks: 1.34GiB\n",
      "2025-05-29 17:15:28.952216: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Total bytes in pool: 2147483648 memory_limit_: 16439525376 available bytes: 14292041728 curr_region_allocation_bytes_: 2147483648\n",
      "2025-05-29 17:15:28.952219: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1133] Stats: \n",
      "Limit:                     16439525376\n",
      "InUse:                      1437431296\n",
      "MaxInUse:                   1437431296\n",
      "NumAllocs:                           6\n",
      "MaxAllocSize:                574972416\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-05-29 17:15:28.952220: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:512] ***************************_______________________*****************************************_________\n",
      "2025-05-29 17:15:28.952232: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at sparse_to_dense_op.cc:119 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1231678,20000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n"
     ]
    }
   ],
   "source": [
    "# --- Compute and cache cosine similarity matrix ---\n",
    "cosine_sim_cache = 'cosine_sim.npy'\n",
    "if os.path.exists(cosine_sim_cache):\n",
    "    cosine_sim = np.load(cosine_sim_cache)\n",
    "    print(f\"Loaded cached cosine similarity matrix: {cosine_sim.shape}\")\n",
    "else:\n",
    "    cosine_sim = compute_cosine_similarity_local(tfidf_matrix)\n",
    "    np.save(cosine_sim_cache, cosine_sim)\n",
    "    print(f\"Computed and saved cosine similarity matrix: {cosine_sim.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AdWUqqM9mzF"
   },
   "outputs": [],
   "source": [
    "# Content-based recommendation\n",
    "def recommend_content(title: str, top_n: int = 5):\n",
    "    idx = df[df['title'].str.lower() == title.lower()].index\n",
    "    if len(idx) == 0:\n",
    "        return f\"Movie '{title}' not found in dataset.\"\n",
    "    idx = idx[0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [i[0] for i in sim_scores[1:top_n+1]]\n",
    "    recommendations = df.iloc[top_indices][['title', 'PrimaryGenre', 'production_companies', 'overview', 'popularity', 'tagline', 'keywords', 'runtime']].copy()\n",
    "    recommendations['Similarity Score'] = [sim_scores[i][1] for i in range(1, top_n+1)]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8wJxWF49m1l"
   },
   "outputs": [],
   "source": [
    "# MiniBatchKMeans for faster clustering\n",
    "num_clusters = 8\n",
    "km = MiniBatchKMeans(n_clusters=num_clusters, random_state=42, batch_size=1000)\n",
    "df['Cluster'] = km.fit_predict(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKVveRkd9m4M"
   },
   "outputs": [],
   "source": [
    "# Evaluate clustering with silhouette score\n",
    "sil_score = silhouette_score(tfidf_matrix, df['Cluster'], metric='cosine', sample_size=2000, random_state=42)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfflXE2x9m6w"
   },
   "outputs": [],
   "source": [
    "# Cluster summary\n",
    "print(\"\\nCluster Sizes and Top Genres:\")\n",
    "cluster_summary = df.groupby('Cluster').agg({\n",
    "    'title': 'count',\n",
    "    'PrimaryGenre': lambda x: x.value_counts().head(3).to_dict()\n",
    "}).rename(columns={'title': 'Count'})\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGH0FFSw9nO3"
   },
   "outputs": [],
   "source": [
    "# Sample movies per cluster\n",
    "print(\"\\nSample Movies per Cluster (Top 3 per cluster):\")\n",
    "for i in range(num_clusters):\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    cluster_movies = df[df['Cluster'] == i][['title', 'PrimaryGenre', 'overview']].head(3)\n",
    "    for _, row in cluster_movies.iterrows():\n",
    "        print(f\"- {row['title']} ({row['PrimaryGenre']}): {row['overview'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7PaR9bU9nnv"
   },
   "outputs": [],
   "source": [
    "# Visualization 1: Silhouette Plot\n",
    "silhouette_vals = silhouette_samples(tfidf_matrix, df['Cluster'], metric='cosine')\n",
    "plt.figure(figsize=(10, 6))\n",
    "y_lower, y_upper = 0, 0\n",
    "for i in range(num_clusters):\n",
    "    cluster_sil_vals = silhouette_vals[df['Cluster'] == i]\n",
    "    cluster_sil_vals.sort()\n",
    "    y_upper += len(cluster_sil_vals)\n",
    "    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_sil_vals, alpha=0.7, label=f'Cluster {i}')\n",
    "    y_lower += len(cluster_sil_vals)\n",
    "plt.axvline(x=sil_score, color='red', linestyle='--', label='Avg Silhouette Score')\n",
    "plt.title('Silhouette Plot for K-Means Clustering')\n",
    "plt.xlabel('Silhouette Coefficient')\n",
    "plt.ylabel('Cluster')\n",
    "plt.legend()\n",
    "plt.savefig('silhouette_plot.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kD6gOl6s-DaO"
   },
   "outputs": [],
   "source": [
    "# Visualization 2: TruncatedSVD for 2D Visualization\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "svd_result = svd.fit_transform(tfidf_matrix)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=svd_result[:, 0], y=svd_result[:, 1], hue=df['Cluster'], palette='Set1', legend='full')\n",
    "plt.title('Movie Clusters (TruncatedSVD Visualization)')\n",
    "plt.xlabel('SVD 1')\n",
    "plt.ylabel('SVD 2')\n",
    "plt.savefig('svd_scatter.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBecwVLd-DdG"
   },
   "outputs": [],
   "source": [
    "# Visualization 3: Genre Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, x='Cluster', hue='PrimaryGenre', palette='Set2')\n",
    "plt.title('Genre Distribution Across Clusters')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.legend(title='Primary Genre', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('genre_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjMza6IN-Dic"
   },
   "outputs": [],
   "source": [
    "# Visualization 4: Word Clouds for Each Cluster\n",
    "for i in range(num_clusters):\n",
    "    cluster_texts = ' '.join(df[df['Cluster'] == i]['text'])\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=30).generate(cluster_texts)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Word Cloud for Cluster {i}')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'wordcloud_cluster_{i}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRJevyn2-Ds1"
   },
   "outputs": [],
   "source": [
    "# Recommendation example\n",
    "sample_movie = 'The Black Viper'\n",
    "print(f\"\\nValidating Recommendations for '{sample_movie}':\")\n",
    "if df['title'].str.lower().eq(sample_movie.lower()).any():\n",
    "    input_movie = df[df['title'].str.lower() == sample_movie.lower()][['title', 'PrimaryGenre', 'production_companies', 'overview']].iloc[0]\n",
    "    print(f\"Input Movie Details:\\n- {input_movie['title']} (PrimaryGenre: {input_movie['PrimaryGenre']}, Production Companies: {input_movie['production_companies']}): {input_movie['overview'][:100]}...\")\n",
    "    recommendations = recommend_content(sample_movie, top_n=5)\n",
    "    if isinstance(recommendations, str):\n",
    "        print(recommendations)\n",
    "    else:\n",
    "        print(\"\\nRecommended Movies:\")\n",
    "        print(recommendations[['title', 'PrimaryGenre', 'production_companies', 'Similarity Score', 'overview']].to_string(index=False))\n",
    "else:\n",
    "    print(f\"Movie '{sample_movie}' not found in dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjM1nFGn-D0s"
   },
   "outputs": [],
   "source": [
    "# Visualization 5: Recommendation Bar Plot\n",
    "idx = df[df['title'].str.lower() == sample_movie.lower()].index\n",
    "if len(idx) > 0:\n",
    "    idx = idx[0]\n",
    "    sim_scores = cosine_sim[idx]\n",
    "    top_indices = np.argsort(sim_scores)[::-1][1:6]\n",
    "    top_scores = sim_scores[top_indices]\n",
    "    top_titles = df.iloc[top_indices]['title'].values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=top_scores, y=top_titles, palette='Blues_r')\n",
    "    plt.title(f'Similarity Scores for Movies Similar to \"{sample_movie}\"')\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Movie Title')\n",
    "    plt.savefig('recommendation_bar_plot.png')\n",
    "    plt.close()\n",
    "else:\n",
    "    print(f\"Cannot plot similarity scores: '{sample_movie}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iR_Vxs3lHdKv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
